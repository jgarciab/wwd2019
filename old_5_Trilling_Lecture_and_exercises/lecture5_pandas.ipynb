{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guest lecture Working with text: Topic Modelling\n",
    "## by Damian Trilling\n",
    "\n",
    "**Amsterdam, 18-1-2018**\n",
    "\n",
    "In this tutorial, I showcase how you can use some automated content analysis techniques (see, e.g., Boumans & Trilling, 2016) using Python and Jupter Notebook.\n",
    "\n",
    "Boumans, J. W., & Trilling, D. (2016). Taking stock of the toolkit: An overview of relevant automated content analysis approaches and techniques for digital journalism scholars. Digital Journalism, 4(1), 8–23. doi:doi.org/10.1080/21670811.2015.1096598\n",
    "\n",
    "## Further ressources:\n",
    "- More slides: https://github.com/damian0604/bdaca/blob/master/courses/turku2017/lecture.pdf\n",
    "- The complete ACA course book: https://github.com/damian0604/bdaca/blob/master/book/bd-aca_book.pdf\n",
    "- Other Notebooks: https://github.com/damian0604/bdaca/tree/master/ipynb\n",
    "- in particular, a longer and more elaborated version of this one: https://github.com/damian0604/bdaca/blob/master/ipynb/ic2s2.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is the Natural Language Toolkit. We need to download some ressources it uses, like lists of stopwords or a POS-tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to install some additional packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user gensim\n",
    "!pip install --user pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably have to restart your kernel now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started!\n",
    "\n",
    "## Import modules\n",
    "Before we start, let's import some modules that we need today. It is good practice to do so at the beginning of a script, so we'll do it right now and not later when we need them. The benefit is that you immediately see if something goes wrong (for instance, because the module is not installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING NOW: 2018-01-21 17:59:55.173807\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from glob import glob\n",
    "from string import punctuation\n",
    "import random\n",
    "import os\n",
    "random.seed(\"ic2s2colgne\")\n",
    "from nltk.sentiment import vader\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "print(\"STARTING NOW:\",str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get more output when estimating topic models:\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "We will use a dataset by Schumacher et al. (2016). From the abstract:\n",
    "> This paper presents EUSpeech, a new dataset of 18,403 speeches from EU leaders (i.e., heads of government in 10 member states, EU commissioners, party leaders in the European Parliament, and ECB and IMF leaders) from 2007 to 2015. These speeches vary in sentiment, topics and ideology, allowing for fine-grained, over-time comparison of representation in the EU. The member states we included are Czech Republic, France, Germany, Greece, Netherlands, Italy, Spain, United Kingdom, Poland and Portugal.\n",
    "\n",
    "Schumacher, G, Schoonvelde, M., Dahiya, T., Traber, D, & de Vries, E. (2016): *EUSpeech: a New Dataset of EU Elite Speeches*. [doi:10.7910/DVN/XPCVEI](http://dx.doi.org/10.7910/DVN/XPCVEI)\n",
    "\n",
    "Download and unpack the following file:\n",
    "```\n",
    "speeches_csv.tar.gz\n",
    "```\n",
    "\n",
    "I already did this for you. Let's have a look at the files we downlaoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ../class_data/EUSpeech/Cleaned_Speeches/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start!\n",
    "Let's retrieve a list of all speeches from one of the files. Of course, we could also loop over all the files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'A roadmap for sustainable recovery'</td>\n",
       "      <td>23-09-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>1404</td>\n",
       "      <td>&lt;p&gt;Ladies and gentlemen,&lt;/p&gt;&lt;p&gt;It is an honour...</td>\n",
       "      <td>ladi gentlemen honour today introduc theme rec...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Speech by Prime Minister Balkenende in New Yor...</td>\n",
       "      <td>22-09-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>840</td>\n",
       "      <td>&lt;p&gt;Ladies and gentlemen,&lt;/p&gt;&lt;p&gt;Today we are lo...</td>\n",
       "      <td>ladi gentlemen today look back ten year sinc m...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Speech at Canadian War Cemetery Bergen op Zoom</td>\n",
       "      <td>06-05-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>545</td>\n",
       "      <td>&lt;p&gt;Prime Minister Harper, Excellencies, honour...</td>\n",
       "      <td>prime minist harper excel honour veteran ladi ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Speech by the Prime Minister at the Fudan Univ...</td>\n",
       "      <td>30-04-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>2753</td>\n",
       "      <td>&lt;p&gt;Ni hao pengyoumen, &lt;/p&gt;&lt;p&gt; Your Excellency,...</td>\n",
       "      <td>ni hao pengyoumen excel professor qin professo...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Speech at the United Nations Memorial Cemetery...</td>\n",
       "      <td>29-04-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>581</td>\n",
       "      <td>&lt;p&gt;Honoured veterans, ladies and gentlemen,&lt;/p...</td>\n",
       "      <td>honour veteran ladi gentlemen today offici fun...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0           1            2  \\\n",
       "0               'A roadmap for sustainable recovery'  23-09-2010  Netherlands   \n",
       "1  Speech by Prime Minister Balkenende in New Yor...  22-09-2010  Netherlands   \n",
       "3     Speech at Canadian War Cemetery Bergen op Zoom  06-05-2010  Netherlands   \n",
       "4  Speech by the Prime Minister at the Fudan Univ...  30-04-2010  Netherlands   \n",
       "5  Speech at the United Nations Memorial Cemetery...  29-04-2010  Netherlands   \n",
       "\n",
       "                 3     4                                                  5  \\\n",
       "0  J.P. Balkenende  1404  <p>Ladies and gentlemen,</p><p>It is an honour...   \n",
       "1  J.P. Balkenende   840  <p>Ladies and gentlemen,</p><p>Today we are lo...   \n",
       "3  J.P. Balkenende   545  <p>Prime Minister Harper, Excellencies, honour...   \n",
       "4  J.P. Balkenende  2753  <p>Ni hao pengyoumen, </p><p> Your Excellency,...   \n",
       "5  J.P. Balkenende   581  <p>Honoured veterans, ladies and gentlemen,</p...   \n",
       "\n",
       "                                                   6   7  \n",
       "0  ladi gentlemen honour today introduc theme rec...  en  \n",
       "1  ladi gentlemen today look back ten year sinc m...  en  \n",
       "3  prime minist harper excel honour veteran ladi ...  en  \n",
       "4  ni hao pengyoumen excel professor qin professo...  en  \n",
       "5  honour veteran ladi gentlemen today offici fun...  en  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_nl = pd.read_csv('../class_data/EUSpeech/Cleaned_Speeches/Speeches_NL_Cleaned.csv',header=None)\n",
    "#To read all files instead\n",
    "#filelist = glob('../class_data/EUSpeech/Cleaned_Speeches/Speeches_*_Cleaned.csv')\n",
    "#speeches = pd.concat([pd.read_csv(file,header=None) for file in filelist])\n",
    "df = speeches_nl.loc[speeches_nl[7]==\"en\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A simple, top-down approach to ACA: Let's count how many times a regular expression is mentioned!\n",
    "\n",
    "\n",
    "\n",
    "Regular Expressions\n",
    "\n",
    "There are a lot of online tutorials explaining regular expressions (and you can read up in my book or on the slides), so I won't go into detail here how to construct one. But let's look at a prototypical usecase: Counting how often something is mentioned in texts. Let's start by examing one single speech:\n",
    "In [ ]:\n",
    "\n",
    "speeches[0]\n",
    "\n",
    "Then we can get a list with all substrings that match the regexp. And, as with any lists, we can calculate its length!\n",
    "In [ ]:\n",
    "\n",
    "re.findall(r\"[Ee]conomy|[Ee]conomic\",speeches[0])\n",
    "\n",
    "In [ ]:\n",
    "\n",
    "len(re.findall(r\"[Ee]conomy|[Ee]conomic\",speeches[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Economy']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[Ee]conomy|[Ee]conomic\",\"Economy is interesting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>economy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'A roadmap for sustainable recovery'</td>\n",
       "      <td>23-09-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>1404</td>\n",
       "      <td>&lt;p&gt;Ladies and gentlemen,&lt;/p&gt;&lt;p&gt;It is an honour...</td>\n",
       "      <td>ladi gentlemen honour today introduc theme rec...</td>\n",
       "      <td>en</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Speech by Prime Minister Balkenende in New Yor...</td>\n",
       "      <td>22-09-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>840</td>\n",
       "      <td>&lt;p&gt;Ladies and gentlemen,&lt;/p&gt;&lt;p&gt;Today we are lo...</td>\n",
       "      <td>ladi gentlemen today look back ten year sinc m...</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Speech at Canadian War Cemetery Bergen op Zoom</td>\n",
       "      <td>06-05-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>545</td>\n",
       "      <td>&lt;p&gt;Prime Minister Harper, Excellencies, honour...</td>\n",
       "      <td>prime minist harper excel honour veteran ladi ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Speech by the Prime Minister at the Fudan Univ...</td>\n",
       "      <td>30-04-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>2753</td>\n",
       "      <td>&lt;p&gt;Ni hao pengyoumen, &lt;/p&gt;&lt;p&gt; Your Excellency,...</td>\n",
       "      <td>ni hao pengyoumen excel professor qin professo...</td>\n",
       "      <td>en</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Speech at the United Nations Memorial Cemetery...</td>\n",
       "      <td>29-04-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>581</td>\n",
       "      <td>&lt;p&gt;Honoured veterans, ladies and gentlemen,&lt;/p...</td>\n",
       "      <td>honour veteran ladi gentlemen today offici fun...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0           1            2  \\\n",
       "0               'A roadmap for sustainable recovery'  23-09-2010  Netherlands   \n",
       "1  Speech by Prime Minister Balkenende in New Yor...  22-09-2010  Netherlands   \n",
       "3     Speech at Canadian War Cemetery Bergen op Zoom  06-05-2010  Netherlands   \n",
       "4  Speech by the Prime Minister at the Fudan Univ...  30-04-2010  Netherlands   \n",
       "5  Speech at the United Nations Memorial Cemetery...  29-04-2010  Netherlands   \n",
       "\n",
       "                 3     4                                                  5  \\\n",
       "0  J.P. Balkenende  1404  <p>Ladies and gentlemen,</p><p>It is an honour...   \n",
       "1  J.P. Balkenende   840  <p>Ladies and gentlemen,</p><p>Today we are lo...   \n",
       "3  J.P. Balkenende   545  <p>Prime Minister Harper, Excellencies, honour...   \n",
       "4  J.P. Balkenende  2753  <p>Ni hao pengyoumen, </p><p> Your Excellency,...   \n",
       "5  J.P. Balkenende   581  <p>Honoured veterans, ladies and gentlemen,</p...   \n",
       "\n",
       "                                                   6   7  economy  \n",
       "0  ladi gentlemen honour today introduc theme rec...  en       10  \n",
       "1  ladi gentlemen today look back ten year sinc m...  en        3  \n",
       "3  prime minist harper excel honour veteran ladi ...  en        0  \n",
       "4  ni hao pengyoumen excel professor qin professo...  en       12  \n",
       "5  honour veteran ladi gentlemen today offici fun...  en        0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"economy\"] = df[5].str.count(r\"[Ee]conomy|[Ee]conomic\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Play around! Look wether you can code your own stuff**\n",
    "If you want to save the results, you can do so with the .to_csv(), .to_excel(), ... methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/myoutput.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A sophisticated, bottom-up approach: LDA topic modelling\n",
    "\n",
    "# From text to features: preprocessing, tokens, n-grams\n",
    "## General approach\n",
    "\n",
    "From a machine-learning perspective, one could argue that all information in a text might be useful information. However, we are interested in getting *interpretable* topics, so even if for instance the use of specific HTML tags would help us distinguising between some documents, we want to get rid of them. More in general, we start by cleaning up a bit to get only 'real' text.\n",
    "\n",
    "### Some typical clean-up steps:\n",
    "*[I realize you did not cover list comprehensions in your corse yet, so the commands below may look a bit like weird magic to you. It is basically a way of writing a for loop in one single line. For now, just accept them as they are ;-) ]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/opt/anaconda/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/anaconda/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/anaconda/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df[5] = df[5].str.lower()\n",
    "df[5] = df[5].str.replace(r\"<p>|</p>\",\" \") #remove html tags\n",
    "regex = re.compile('[%s]' % re.escape(punctuation))\n",
    "df[5] = df[5].str.replace(regex,\"\") #remove punctuation\n",
    "df[5] = df[5].str.replace(r\" +\",\" \") #remove double spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first speech to check everything's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ladies and gentlemen it is an honour to be he...\n",
       "1     ladies and gentlemen today we are looking bac...\n",
       "3     prime minister harper excellencies honoured v...\n",
       "4     ni hao pengyoumen your excellency professor q...\n",
       "5     honoured veterans ladies and gentlemen today ...\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[5].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as with other unsupervised machine learning techniques, we are not really interesting a long string of text. We rather want to have each document being represented by a set of *features*. To this end, `gensim` has a finciton `doc2bow` that converts a list of words (tokens) to `(token_id, token_count)` tuples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "We immediately see that the result of our first LDA doesn't make much sense: We see only stopwords. \n",
    "\n",
    "\n",
    "### Explicit stopword removal\n",
    "The most straightforward approach is to use a pre-existing list with stopwords, possibly with the addition of some own, case-specific words. We then split up each speech in words, and only if a word is not on the stopwordlist, we keep it and join it with the previous and next word using a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/opt/anaconda/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>economy</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'A roadmap for sustainable recovery'</td>\n",
       "      <td>23-09-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>1404</td>\n",
       "      <td>ladies and gentlemen it is an honour to be he...</td>\n",
       "      <td>ladi gentlemen honour today introduc theme rec...</td>\n",
       "      <td>en</td>\n",
       "      <td>10</td>\n",
       "      <td>[ladies, gentlemen, honour, today, introduce, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Speech by Prime Minister Balkenende in New Yor...</td>\n",
       "      <td>22-09-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>840</td>\n",
       "      <td>ladies and gentlemen today we are looking bac...</td>\n",
       "      <td>ladi gentlemen today look back ten year sinc m...</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "      <td>[ladies, gentlemen, today, looking, back, ten,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Speech at Canadian War Cemetery Bergen op Zoom</td>\n",
       "      <td>06-05-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>545</td>\n",
       "      <td>prime minister harper excellencies honoured v...</td>\n",
       "      <td>prime minist harper excel honour veteran ladi ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>[prime, minister, harper, excellencies, honour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Speech by the Prime Minister at the Fudan Univ...</td>\n",
       "      <td>30-04-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>2753</td>\n",
       "      <td>ni hao pengyoumen your excellency professor q...</td>\n",
       "      <td>ni hao pengyoumen excel professor qin professo...</td>\n",
       "      <td>en</td>\n",
       "      <td>12</td>\n",
       "      <td>[ni, hao, pengyoumen, excellency, professor, q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Speech at the United Nations Memorial Cemetery...</td>\n",
       "      <td>29-04-2010</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>J.P. Balkenende</td>\n",
       "      <td>581</td>\n",
       "      <td>honoured veterans ladies and gentlemen today ...</td>\n",
       "      <td>honour veteran ladi gentlemen today offici fun...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>[honoured, veterans, ladies, gentlemen, today,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0           1            2  \\\n",
       "0               'A roadmap for sustainable recovery'  23-09-2010  Netherlands   \n",
       "1  Speech by Prime Minister Balkenende in New Yor...  22-09-2010  Netherlands   \n",
       "3     Speech at Canadian War Cemetery Bergen op Zoom  06-05-2010  Netherlands   \n",
       "4  Speech by the Prime Minister at the Fudan Univ...  30-04-2010  Netherlands   \n",
       "5  Speech at the United Nations Memorial Cemetery...  29-04-2010  Netherlands   \n",
       "\n",
       "                 3     4                                                  5  \\\n",
       "0  J.P. Balkenende  1404   ladies and gentlemen it is an honour to be he...   \n",
       "1  J.P. Balkenende   840   ladies and gentlemen today we are looking bac...   \n",
       "3  J.P. Balkenende   545   prime minister harper excellencies honoured v...   \n",
       "4  J.P. Balkenende  2753   ni hao pengyoumen your excellency professor q...   \n",
       "5  J.P. Balkenende   581   honoured veterans ladies and gentlemen today ...   \n",
       "\n",
       "                                                   6   7  economy  \\\n",
       "0  ladi gentlemen honour today introduc theme rec...  en       10   \n",
       "1  ladi gentlemen today look back ten year sinc m...  en        3   \n",
       "3  prime minist harper excel honour veteran ladi ...  en        0   \n",
       "4  ni hao pengyoumen excel professor qin professo...  en       12   \n",
       "5  honour veteran ladi gentlemen today offici fun...  en        0   \n",
       "\n",
       "                                               clean  \n",
       "0  [ladies, gentlemen, honour, today, introduce, ...  \n",
       "1  [ladies, gentlemen, today, looking, back, ten,...  \n",
       "3  [prime, minister, harper, excellencies, honour...  \n",
       "4  [ni, hao, pengyoumen, excellency, professor, q...  \n",
       "5  [honoured, veterans, ladies, gentlemen, today,...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystopwords = set(stopwords.words('english')) # use default NLTK stopword list; alternatively:\n",
    "# mystopwords = set(open('mystopwordfile.txt').readlines())  #read stopword list from a textfile with one stopword per line\n",
    "df[\"clean\"] = df[5].str.split(\" \")\n",
    "df[\"clean\"] = df[\"clean\"].apply(lambda x: [word for word in x if ((word not in mystopwords) and (len(word)>1))])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUUUT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF scores instead of word counts as features\n",
    "Explicitly removing stopwords is a common practice and often very useful. We shouldn't forget, though, that there are some problematic aspects to it as well\n",
    "- It is kind of arbitrary what is on the stopword list and what now\n",
    "- Depending on the research question one is interested in, it might differ what words are 'meaningful'\n",
    "- Although the list is meant to consist of words that occur with a high frequency in all texts, it is not based on actual frequencies in the corpus but set a priori.\n",
    "A different approach would therefore be to simply use (a) the frequency of each word in the corpus and (b) the number of documents in which the document occurs. \n",
    "In other words: If we use tf-idf scores (term frequency weighed by the inverse document frequncy) instead of raw word counts as featues, the stopwords should disappear automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-21 18:19:56,676 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-01-21 18:19:56,754 : INFO : built Dictionary(7383 unique tokens: ['uk', 'awakening', 'adapt', 'days', 'teaches']...) from 132 documents (total 57614 corpus positions)\n",
      "2018-01-21 18:19:56,755 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-17eac02501ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#this we have done in cla         # convert all strings to list of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mldainput_m1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"clean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                       \u001b[0;31m# assign a token_id to each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mid2word_m1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldainput_m1\u001b[0m\u001b[0;34m)\u001b[0m                       \u001b[0;31m# assign a token_id to each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mldacorpus_m1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid2word_m1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mldainput_m1\u001b[0m\u001b[0;34m]\u001b[0m       \u001b[0;31m# represent each speech by (token_id, token_count) tuples\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtfidfcorpus_m1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldacorpus_m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.5/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.5/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         logger.info(\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.5/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# to avoid messing things up, I'll call all objects relating to our first model _m1\",\n",
    "#this we have done in cla         # convert all strings to list of words\n",
    "ldainput_m1 = corpora.Dictionary(df[\"clean\"].tolist())                       # assign a token_id to each word\n",
    "id2word_m1 = corpora.Dictionary(ldainput_m1)                       # assign a token_id to each word\n",
    "ldacorpus_m1 = [id2word_m1.doc2bow(doc) for doc in ldainput_m1]       # represent each speech by (token_id, token_count) tuples\"\n",
    "tfidfcorpus_m1 = models.TfidfModel(ldacorpus_m1)\n",
    "lda_m1 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m1[ldacorpus_m1],id2word=id2word_m1,num_topics=10)\n",
    "lda_m1.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering extremes\n",
    "Just as we don't want to include words that are all over the place and do little to distinguish documents, we also do not want to include words that virtually never occur. If among millions of words, a word occurs exactly one time, it might be simply a spelling mistake. But even if it is not, it does not help us to infer topics across documents. \n",
    "\n",
    "Also in purely pragmatic terms, it makes sense to remove unneccessary features to speed up the analysis process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_m4 = corpora.Dictionary(ldainput_m1)        # reuse input from M1     \n",
    "\n",
    "id2word_m4.filter_extremes(no_below=5, no_above=0.5)   # do not consider all words that occur in less than n=5 documents\n",
    "                                                    # or in more than 50% of all documents.\n",
    "\n",
    "ldacorpus_m4 = [id2word_m4.doc2bow(doc) for doc in ldainput_m1]\n",
    "tfidfcorpus_m4 = models.TfidfModel(ldacorpus_m4)\n",
    "lda_m4 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m4[ldacorpus_m4],id2word=id2word_m4,num_topics=10)\n",
    "lda_m4.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other preprocessing ideas\n",
    "\n",
    "### Stemming\n",
    "Stemming can be useful to avoid that 'economics', 'economic', and 'economy' are seen as different concepts by the topic model. In practice, however, standard stemming algorithms are far from perfect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "speeches_nl_stemmed = [\" \".join([stemmer.stem(word) for word in speech.split()]) for speech in speeches_nl]\n",
    "speeches_nl_stemmed[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and retaining only nouns and adjectives\n",
    "Depending on the specific use case at hand, one might discover that some parts of speech (POS) are more informative than others. We could, for instance, create a topic model based on only the nouns and adjectives in a text, disregarding everything else. \n",
    "Look at the NLTK documentation to find out what each code means (e.g., 'NN' is 'noun') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_nl_nounsadj=[]\n",
    "for speech in speeches_nl:\n",
    "    tokens = nltk.word_tokenize(speech)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    cleanspeech = \"\"\n",
    "    for element in tagged:\n",
    "        if element[1] in ('NN','NNP','JJ'):\n",
    "            cleanspeech=cleanspeech+element[0]+\" \"\n",
    "    speeches_nl_nounsadj.append(cleanspeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_nl_nounsadj[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using ngrams as features\n",
    "Topic models follow a bag-of-words approach, meaning they do not take word order into account. However, sometimes we want to be able to do so to a limited extend: The \"white house\" is something else than a \"house with a white wall\", even though both strings contain the words 'white' and 'house'. We can do so by joining adjacent words together in so-called bigrams (or trigrams, if we take three words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_nl_bigrams = [[\"_\".join(tup) for tup in nltk.ngrams(speech.split(),2)] for speech in speeches_nl_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we want both unigrams and bigrams in the feature set?\n",
    "assert len(speeches_nl_clean)==len(speeches_nl_bigrams)\n",
    "speeches_nl_uniandbigrams = []\n",
    "for a,b in zip([speech.split() for speech in speeches_nl_clean],speeches_nl_bigrams):\n",
    "    speeches_nl_uniandbigrams.append(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(speeches_nl_uniandbigrams[6]),len(speeches_nl_bigrams[6]),len(speeches_nl_clean[6].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_m5 = corpora.Dictionary(speeches_nl_uniandbigrams)                       \n",
    "id2word_m5.filter_extremes(no_below=5, no_above=0.5)\n",
    "ldacorpus_m5 = [id2word_m5.doc2bow(doc) for doc in speeches_nl_uniandbigrams]\n",
    "tfidfcorpus_m5 = models.TfidfModel(ldacorpus_m5)\n",
    "lda_m5 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m5[ldacorpus_m5],id2word=id2word_m5,num_topics=10)\n",
    "lda_m5.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up\n",
    "While there are different ways to achieve the desired results and different modules to help you with it (writing from scratch, NLTK, but also gensim.utils), these are some steps to consider when transforming texts to feature sets for topic modeling (recall that not all of them might be neccessary of even diserable, depending on the use case):\n",
    "\n",
    "- transforming to lowercase\n",
    "- removing stopwords\n",
    "- stemming\n",
    "- POS-tagging (and removing unwanted elements)\n",
    "- filtering extremely common and extremely uncommon words\n",
    "- ngrams and/or unigrams as features?\n",
    "- raw frequencies or TF-IDF scores as features?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualizing and interactively exploring topic models\n",
    "A great tool for interactively exploring topicmodels is pyLDAvis.\n",
    "pyLDAvis can estimate its own topic models, but it als has a nice function called `gensim.prepare`, which you can use to visualize the model you already estimated with gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.gensim.prepare(lda_m5,ldacorpus_m5,id2word_m5)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. What’s the next step in the pipeline? Using the results of a topic model\n",
    "\n",
    "Until know, we have mainly considered the interpretation of the topics themselves. While it can indeed be interesting to use topic models to summarize and interpret large corpora, this is usually not where social scientists stop: We want to relate the topics back to documents to say something about which topics occur in which documents.\n",
    "\n",
    "## Saving topic scores to a file\n",
    "Somewhat similar to factor analysis and principal component analysis, where one can also store factor scores that indicate how high a specific case scores on each of the factors that were identified, for each document, we can estimate a score for each of the topics we identified.\n",
    "\n",
    "To do so, we can simply call the `.inference()` method on the model we estimated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresperdoc=lda_m5.inference(ldacorpus_m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scoresperdoc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do a lot of different stuff with the resulting matrix, in which each row represents one of the documents and each row consists of one score for each topic.\n",
    "For example, we just could create a tab-separated file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topicscores.tsv\",\"w\",encoding=\"utf-8\") as fo:\n",
    "    for row in scoresperdoc[0]:\n",
    "       fo.write(\"\\t\".join([\"{:0.3f}\".format(score) for score in row]))\n",
    "       fo.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or we put it into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(scoresperdoc[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these scores are extremely skewed. Maybe we just want to know which topics score really high? Let's recode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df.applymap(lambda x: int(x>10))\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a heatmap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning alpha and eta\n",
    "different parameters. From docstring:\n",
    "```\n",
    "`alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
    "(theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
    "1.0/num_topics prior.\n",
    "\n",
    "`alpha` can be set to an explicit array = prior of your choice. It also\n",
    "support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
    "normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
    "prior directly from your data.\n",
    "\n",
    "`eta` can be a scalar for a symmetric prior over topic/word\n",
    "distributions, or a matrix of shape num_topics x num_words, which can\n",
    "be used to impose asymmetric priors over the word distribution on a\n",
    "per-topic basis. This may be useful if you want to seed certain topics\n",
    "with particular words by boosting the priors for those words.  It also\n",
    "supports the special value 'auto', which learns an asymmetric prior\n",
    "directly from your data.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example for different specification: repeat analysis 10 times, while learning alpha and eta from the data \n",
    "# instead of using 1/number of topics as defailt\n",
    "lda_m6 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m5[ldacorpus_m5],id2word=id2word_m5,num_topics=50, alpha='auto', eta = 'symmetric',passes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.gensim.prepare(lda_m6,ldacorpus_m5,id2word_m5)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Exercises\n",
    "\n",
    "**Choose different input data, for instance the speeches from a different country. Construct a topic model.**\n",
    "\n",
    "1. Reflect on what choices you made and why:\n",
    "\n",
    "- Which, if any preprocessing?\n",
    "- Which features? (e.g., bigram counts; unigrams represented as tf-idf scores; ...)\n",
    "- Which other model specifications?\n",
    "\n",
    "2. What did your analysis tell you about the data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How further?\n",
    "\n",
    "You got interested an want to continue with ACA?\n",
    "\n",
    "Maybe these ressources are helpful:\n",
    "\n",
    "\n",
    "- http://github.com/damian0604/bdaca (in particular the book)\n",
    "- http://damiantrilling.net/tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINISHED RUNNING:\",str(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
