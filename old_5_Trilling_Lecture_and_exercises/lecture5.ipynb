{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guest lecture Working with text: Topic Modelling\n",
    "## by Damian Trilling\n",
    "\n",
    "**Amsterdam, 18-1-2018**\n",
    "\n",
    "In this tutorial, I showcase how you can use some automated content analysis techniques (see, e.g., Boumans & Trilling, 2016) using Python and Jupter Notebook.\n",
    "\n",
    "Boumans, J. W., & Trilling, D. (2016). Taking stock of the toolkit: An overview of relevant automated content analysis approaches and techniques for digital journalism scholars. Digital Journalism, 4(1), 8â€“23. doi:doi.org/10.1080/21670811.2015.1096598\n",
    "\n",
    "## Further ressources:\n",
    "- More slides: https://github.com/damian0604/bdaca/blob/master/courses/turku2017/lecture.pdf\n",
    "- The complete ACA course book: https://github.com/damian0604/bdaca/blob/master/book/bd-aca_book.pdf\n",
    "- Other Notebooks: https://github.com/damian0604/bdaca/tree/master/ipynb\n",
    "- in particular, a longer and more elaborated version of this one: https://github.com/damian0604/bdaca/blob/master/ipynb/ic2s2.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is the Natural Language Toolkit. We need to download some ressources it uses, like lists of stopwords or a POS-tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to install some additional packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user gensim\n",
    "!pip install --user pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably have to restart your kernel now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started!\n",
    "\n",
    "## Import modules\n",
    "Before we start, let's import some modules that we need today. It is good practice to do so at the beginning of a script, so we'll do it right now and not later when we need them. The benefit is that you immediately see if something goes wrong (for instance, because the module is not installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from glob import glob\n",
    "from string import punctuation\n",
    "import random\n",
    "random.seed(\"ic2s2colgne\")\n",
    "from nltk.sentiment import vader\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "print(\"STARTING NOW:\",str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get more output when estimating topic models:\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "We will use a dataset by Schumacher et al. (2016). From the abstract:\n",
    "> This paper presents EUSpeech, a new dataset of 18,403 speeches from EU leaders (i.e., heads of government in 10 member states, EU commissioners, party leaders in the European Parliament, and ECB and IMF leaders) from 2007 to 2015. These speeches vary in sentiment, topics and ideology, allowing for fine-grained, over-time comparison of representation in the EU. The member states we included are Czech Republic, France, Germany, Greece, Netherlands, Italy, Spain, United Kingdom, Poland and Portugal.\n",
    "\n",
    "Schumacher, G, Schoonvelde, M., Dahiya, T., Traber, D, & de Vries, E. (2016): *EUSpeech: a New Dataset of EU Elite Speeches*. [doi:10.7910/DVN/XPCVEI](http://dx.doi.org/10.7910/DVN/XPCVEI)\n",
    "\n",
    "Download and unpack the following file:\n",
    "```\n",
    "speeches_csv.tar.gz\n",
    "```\n",
    "\n",
    "I already did this for you. Let's have a look at the files we downlaoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ../class_data/EUSpeech/Cleaned_Speeches/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start!\n",
    "Let's retrieve a list of all speeches from one of the files. Of course, we could also loop over all the files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of all speeches, to speed up things, we are focusing on speeches from the Netherlands only.\n",
    "# If you want all speeches, use the second line instead\n",
    "filelist = glob('../class_data/EUSpeech/Cleaned_Speeches/Speeches_NL_Cleaned.csv')\n",
    "#filelist = glob('../class_data/EUSpeech/Cleaned_Speeches/Speeches_*_Cleaned.csv')\n",
    "print(filelist)\n",
    "speeches_nl=[]\n",
    "for fn in filelist:\n",
    "    with open(fn) as fi:\n",
    "        reader=csv.reader(fi)\n",
    "        for row in reader:\n",
    "            if row[7]=='en':   # only include english-language speches; we might as well choose 'nl' or 'fr'\n",
    "                speeches_nl.append(row[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed things up, we could also sample some speeches. \n",
    "# speeches = random.sample(speeches,100)\n",
    "# len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_nl[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A simple, top-down approach to ACA: Let's count how many times a regular expression is mentioned!\n",
    "\n",
    "\n",
    "\n",
    "Regular Expressions\n",
    "\n",
    "There are a lot of online tutorials explaining regular expressions (and you can read up in my book or on the slides), so I won't go into detail here how to construct one. But let's look at a prototypical usecase: Counting how often something is mentioned in texts. Let's start by examing one single speech:\n",
    "In [ ]:\n",
    "\n",
    "speeches[0]\n",
    "\n",
    "Then we can get a list with all substrings that match the regexp. And, as with any lists, we can calculate its length!\n",
    "In [ ]:\n",
    "\n",
    "re.findall(r\"[Ee]conomy|[Ee]conomic\",speeches[0])\n",
    "\n",
    "In [ ]:\n",
    "\n",
    "len(re.findall(r\"[Ee]conomy|[Ee]conomic\",speeches[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"[Ee]conomy|[Ee]conomic\",speeches_nl[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':speeches_nl})\n",
    "df['economy'] = df['text'].map(lambda x: re.findall(r\"[Ee]conomy|[Ee]conomic\", x)).map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Play around! Look wether you can code your own stuff**\n",
    "If you want to save the results, you can do so with the .to_csv(), .to_excel(), ... methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('myoutput.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A sophisticated, bottom-up approach: LDA topic modelling\n",
    "\n",
    "# From text to features: preprocessing, tokens, n-grams\n",
    "## General approach\n",
    "\n",
    "From a machine-learning perspective, one could argue that all information in a text might be useful information. However, we are interested in getting *interpretable* topics, so even if for instance the use of specific HTML tags would help us distinguising between some documents, we want to get rid of them. More in general, we start by cleaning up a bit to get only 'real' text.\n",
    "\n",
    "### Some typical clean-up steps:\n",
    "*[I realize you did not cover list comprehensions in your corse yet, so the commands below may look a bit like weird magic to you. It is basically a way of writing a for loop in one single line. For now, just accept them as they are ;-) ]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_nl=[speech.replace('<p>',' ').replace('</p>',' ') for speech in speeches_nl]   #remove HTML tags\n",
    "speeches_nl=[\"\".join([l for l in speech if l not in punctuation]) for speech in speeches_nl]  #remove punctuation\n",
    "speeches_nl=[speech.lower() for speech in speeches_nl]  # convert to lower case\n",
    "speeches_nl=[\" \".join(speech.split()) for speech in speeches_nl]   # remove double spaces by splitting the strings into words and joining these words again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first speech to check everything's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_nl[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as with other unsupervised machine learning techniques, we are not really interesting a long string of text. We rather want to have each document being represented by a set of *features*. To this end, `gensim` has a finciton `doc2bow` that converts a list of words (tokens) to `(token_id, token_count)` tuples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "We immediately see that the result of our first LDA doesn't make much sense: We see only stopwords. \n",
    "\n",
    "\n",
    "### Explicit stopword removal\n",
    "The most straightforward approach is to use a pre-existing list with stopwords, possibly with the addition of some own, case-specific words. We then split up each speech in words, and only if a word is not on the stopwordlist, we keep it and join it with the previous and next word using a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwords = set(stopwords.words('english')) # use default NLTK stopword list; alternatively:\n",
    "# mystopwords = set(open('mystopwordfile.txt').readlines())  #read stopword list from a textfile with one stopword per line\n",
    "\n",
    "speeches_nl_clean = [\" \".join([w for w in speech.split() if w not in mystopwords]) for speech in speeches_nl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_nl_clean[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUUUT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF scores instead of word counts as features\n",
    "Explicitly removing stopwords is a common practice and often very useful. We shouldn't forget, though, that there are some problematic aspects to it as well\n",
    "- It is kind of arbitrary what is on the stopword list and what now\n",
    "- Depending on the research question one is interested in, it might differ what words are 'meaningful'\n",
    "- Although the list is meant to consist of words that occur with a high frequency in all texts, it is not based on actual frequencies in the corpus but set a priori.\n",
    "A different approach would therefore be to simply use (a) the frequency of each word in the corpus and (b) the number of documents in which the document occurs. \n",
    "In other words: If we use tf-idf scores (term frequency weighed by the inverse document frequncy) instead of raw word counts as featues, the stopwords should disappear automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid messing things up, I'll call all objects relating to our first model _m1\",\n",
    "ldainput_m1 = [speech.split() for speech in speeches_nl]           # convert all strings to list of words\n",
    "id2word_m1 = corpora.Dictionary(ldainput_m1)                       # assign a token_id to each word\n",
    "ldacorpus_m1 = [id2word_m1.doc2bow(doc) for doc in ldainput_m1]       # represent each speech by (token_id, token_count) tuples\"\n",
    "tfidfcorpus_m1 = models.TfidfModel(ldacorpus_m1)\n",
    "lda_m1 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m1[ldacorpus_m1],id2word=id2word_m1,num_topics=10)\n",
    "lda_m1.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering extremes\n",
    "Just as we don't want to include words that are all over the place and do little to distinguish documents, we also do not want to include words that virtually never occur. If among millions of words, a word occurs exactly one time, it might be simply a spelling mistake. But even if it is not, it does not help us to infer topics across documents. \n",
    "\n",
    "Also in purely pragmatic terms, it makes sense to remove unneccessary features to speed up the analysis process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_m4 = corpora.Dictionary(ldainput_m1)        # reuse input from M1     \n",
    "\n",
    "id2word_m4.filter_extremes(no_below=5, no_above=0.5)   # do not consider all words that occur in less than n=5 documents\n",
    "                                                    # or in more than 50% of all documents.\n",
    "\n",
    "ldacorpus_m4 = [id2word_m4.doc2bow(doc) for doc in ldainput_m1]\n",
    "tfidfcorpus_m4 = models.TfidfModel(ldacorpus_m4)\n",
    "lda_m4 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m4[ldacorpus_m4],id2word=id2word_m4,num_topics=10)\n",
    "lda_m4.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other preprocessing ideas\n",
    "\n",
    "### Stemming\n",
    "Stemming can be useful to avoid that 'economics', 'economic', and 'economy' are seen as different concepts by the topic model. In practice, however, standard stemming algorithms are far from perfect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "speeches_nl_stemmed = [\" \".join([stemmer.stem(word) for word in speech.split()]) for speech in speeches_nl]\n",
    "speeches_nl_stemmed[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and retaining only nouns and adjectives\n",
    "Depending on the specific use case at hand, one might discover that some parts of speech (POS) are more informative than others. We could, for instance, create a topic model based on only the nouns and adjectives in a text, disregarding everything else. \n",
    "Look at the NLTK documentation to find out what each code means (e.g., 'NN' is 'noun') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_nl_nounsadj=[]\n",
    "for speech in speeches_nl:\n",
    "    tokens = nltk.word_tokenize(speech)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    cleanspeech = \"\"\n",
    "    for element in tagged:\n",
    "        if element[1] in ('NN','NNP','JJ'):\n",
    "            cleanspeech=cleanspeech+element[0]+\" \"\n",
    "    speeches_nl_nounsadj.append(cleanspeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_nl_nounsadj[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using ngrams as features\n",
    "Topic models follow a bag-of-words approach, meaning they do not take word order into account. However, sometimes we want to be able to do so to a limited extend: The \"white house\" is something else than a \"house with a white wall\", even though both strings contain the words 'white' and 'house'. We can do so by joining adjacent words together in so-called bigrams (or trigrams, if we take three words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_nl_bigrams = [[\"_\".join(tup) for tup in nltk.ngrams(speech.split(),2)] for speech in speeches_nl_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we want both unigrams and bigrams in the feature set?\n",
    "assert len(speeches_nl_clean)==len(speeches_nl_bigrams)\n",
    "speeches_nl_uniandbigrams = []\n",
    "for a,b in zip([speech.split() for speech in speeches_nl_clean],speeches_nl_bigrams):\n",
    "    speeches_nl_uniandbigrams.append(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(speeches_nl_uniandbigrams[6]),len(speeches_nl_bigrams[6]),len(speeches_nl_clean[6].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_m5 = corpora.Dictionary(speeches_nl_uniandbigrams)                       \n",
    "id2word_m5.filter_extremes(no_below=5, no_above=0.5)\n",
    "ldacorpus_m5 = [id2word_m5.doc2bow(doc) for doc in speeches_nl_uniandbigrams]\n",
    "tfidfcorpus_m5 = models.TfidfModel(ldacorpus_m5)\n",
    "lda_m5 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m5[ldacorpus_m5],id2word=id2word_m5,num_topics=10)\n",
    "lda_m5.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up\n",
    "While there are different ways to achieve the desired results and different modules to help you with it (writing from scratch, NLTK, but also gensim.utils), these are some steps to consider when transforming texts to feature sets for topic modeling (recall that not all of them might be neccessary of even diserable, depending on the use case):\n",
    "\n",
    "- transforming to lowercase\n",
    "- removing stopwords\n",
    "- stemming\n",
    "- POS-tagging (and removing unwanted elements)\n",
    "- filtering extremely common and extremely uncommon words\n",
    "- ngrams and/or unigrams as features?\n",
    "- raw frequencies or TF-IDF scores as features?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualizing and interactively exploring topic models\n",
    "A great tool for interactively exploring topicmodels is pyLDAvis.\n",
    "pyLDAvis can estimate its own topic models, but it als has a nice function called `gensim.prepare`, which you can use to visualize the model you already estimated with gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.gensim.prepare(lda_m5,ldacorpus_m5,id2word_m5)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Whatâ€™s the next step in the pipeline? Using the results of a topic model\n",
    "\n",
    "Until know, we have mainly considered the interpretation of the topics themselves. While it can indeed be interesting to use topic models to summarize and interpret large corpora, this is usually not where social scientists stop: We want to relate the topics back to documents to say something about which topics occur in which documents.\n",
    "\n",
    "## Saving topic scores to a file\n",
    "Somewhat similar to factor analysis and principal component analysis, where one can also store factor scores that indicate how high a specific case scores on each of the factors that were identified, for each document, we can estimate a score for each of the topics we identified.\n",
    "\n",
    "To do so, we can simply call the `.inference()` method on the model we estimated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresperdoc=lda_m5.inference(ldacorpus_m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scoresperdoc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do a lot of different stuff with the resulting matrix, in which each row represents one of the documents and each row consists of one score for each topic.\n",
    "For example, we just could create a tab-separated file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topicscores.tsv\",\"w\",encoding=\"utf-8\") as fo:\n",
    "    for row in scoresperdoc[0]:\n",
    "       fo.write(\"\\t\".join([\"{:0.3f}\".format(score) for score in row]))\n",
    "       fo.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or we put it into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(scoresperdoc[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these scores are extremely skewed. Maybe we just want to know which topics score really high? Let's recode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df.applymap(lambda x: int(x>10))\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a heatmap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning alpha and eta\n",
    "different parameters. From docstring:\n",
    "```\n",
    "`alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
    "(theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
    "1.0/num_topics prior.\n",
    "\n",
    "`alpha` can be set to an explicit array = prior of your choice. It also\n",
    "support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
    "normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
    "prior directly from your data.\n",
    "\n",
    "`eta` can be a scalar for a symmetric prior over topic/word\n",
    "distributions, or a matrix of shape num_topics x num_words, which can\n",
    "be used to impose asymmetric priors over the word distribution on a\n",
    "per-topic basis. This may be useful if you want to seed certain topics\n",
    "with particular words by boosting the priors for those words.  It also\n",
    "supports the special value 'auto', which learns an asymmetric prior\n",
    "directly from your data.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example for different specification: repeat analysis 10 times, while learning alpha and eta from the data \n",
    "# instead of using 1/number of topics as defailt\n",
    "lda_m6 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m5[ldacorpus_m5],id2word=id2word_m5,num_topics=50, alpha='auto', eta = 'symmetric',passes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.gensim.prepare(lda_m6,ldacorpus_m5,id2word_m5)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Exercises\n",
    "\n",
    "**Choose different input data, for instance the speeches from a different country. Construct a topic model.**\n",
    "\n",
    "1. Reflect on what choices you made and why:\n",
    "\n",
    "- Which, if any preprocessing?\n",
    "- Which features? (e.g., bigram counts; unigrams represented as tf-idf scores; ...)\n",
    "- Which other model specifications?\n",
    "\n",
    "2. What did your analysis tell you about the data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How further?\n",
    "\n",
    "You got interested an want to continue with ACA?\n",
    "\n",
    "Maybe these ressources are helpful:\n",
    "\n",
    "\n",
    "- http://github.com/damian0604/bdaca (in particular the book)\n",
    "- http://damiantrilling.net/tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINISHED RUNNING:\",str(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
